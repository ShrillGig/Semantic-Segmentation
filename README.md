# Semantic-Segmentation

In this section I shortly intriduce the semantic segmentation neural network models. Right now there are only 3 models: U-Net, LinkNet, PSPNet. 

Semantic segmentation is a computer vision method that not only detects objects in images but also determines their exact spatial location by classifying each pixel. This technique is useful when an image needs to be divided into multiple categories (multiclass classification) rather than just distinguishing between two classes (binary classification). Convolutional neural networks (CNNs) were specifically developed for image processing and have shown higher accuracy than traditional multilayer perceptrons (MLPs), while also requiring fewer trainable parameters. A major breakthrough in CNN-based image classification happened with the introduction of AlexNet, which won the ImageNet competition in 2012. Later, more advanced models were developed, including VGG, ResNet, and GoogleNet. However, these models were mostly designed for whole-image classification rather than pixel-level seg-
mentation. For semantic segmentation, the U-Net architecture was introduced. Originally designed for medical image analysis, U-Net has an encoder-decoder structure, where the encoder extracts image features, and the decoder reconstructs spatial information to produce a segmentation map. Other architectures have also been proposed, such as LinkNet, which is optimized for real-time applications, and PSPNet, which uses a pyramidal pooling structure (PPM) to capture both local and global image features.

U-Net was originally introduced in 2015 at the Cell Segmentation Competition. It was initially used for medical image segmentation, but over time it has been used in other applications due to its encoder-decoder architecture (Figure 1). It consists of four blocks, in each of which two convolutions are sequentially applied to extract increasinglydetailed features, followed by a Max Pooling operation to halve the feature map size. Then all the obtained feature maps are passed already to the decoder through the bottleneck - the deepest part of the network, where the maximum compression of information takes place before it will be restored in
the decoder. The decoder consists of blocks similar to those of the encoder, but instead of Max Pooling a transposed convolution is used, which doubles the spatial size of the feature map. Through skip connection mechanisms, the features from the encoder are concatenated with the corresponding decoder level, which allows restoring spatial details lost during compression. The U-Net output generates a semantic mask corresponding to the original image dimensions. To reduce the computational complexityof the model, a lightweight U-Net is used, in which the number of extracted feature maps is reduced by a factor of 4. This modification allows to preserve the efficiency of segmentation while reducing the memory cost and training time.

![image](https://github.com/user-attachments/assets/ed5950a2-1927-4612-b98b-14045c5dfd50)

LinkNet was introduced in 2017 as a semantic segmentation model designed for real-time image processing (Figure 2). The main idea is to efficiently transfer the extracted features between encoder and decoder layers using skip connections, which improves the accuracy and reduces the loss of spatial information. The LinkNet encoder starts with an initial block that applies convolution of the input image using a 7x7 kernel size and a 2x2 stride, and then performs max-pooling in a 3x3 area and a 2x2 stride. Next come 4 encoder blocks, each consisting of two residual blocks. With each level, the number of feature map channels is doubled, starting at 64. The decoder block, on the other hand, consists of two convolutions and one transpose convolution. With each decoder level, the number of feature map channels is also halved, starting at 512. The final block is responsible for bringing the feature map to the desired number of classes. With the transpose convolution of the 3x3 kernel reduces the feature map from 64 to 32, and then with the 2x2 kernel brings the size to the number of classes present in the dataset.

![image](https://github.com/user-attachments/assets/ba736ee0-88e4-49bc-b6ce-2734a8c06527)

Semantic Segmentation model Pyramid Scene Parcing Network (PSPNet) was submitted by Chinese University of Hong Kong and won the 2016 ImageNet Challenge. The first challenge of PSPNet was to improve Fully Convolutional network (FCN) model. With Pyramid Pooling Module (PPM), the neural network is able to capture both global and local features, which is especially special for complex images (Figure 3). First, the input image is passed through the CNN where all possible features are extracted. Then from the final layer we pass them to the PPM. This module decomposes the feature maps into several layers with different resolutions, each of which extracts certain features. PPM creates a pyramid by performing Average pooling with different resolutions (1x1, 2x2, 3x3, 6x6). At the 1x1 level, the feature map is compressed into a single value for each channel to provide a global image context. At the 6x6 level, on the other hand, more localized features are extracted. After processing, the feature maps are combined using concatenation and the resulting maps are passed through a segmentation enhancement layer with a 3x3 convolution kernel.

![image](https://github.com/user-attachments/assets/32c352e9-e1b7-4347-b9b8-2e18d6c6d631)



